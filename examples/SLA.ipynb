{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882bc1f2-d2c4-4300-8c95-1cd22403adc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Literal, Optional, Dict\n",
    "import math\n",
    "from scipy.stats import beta as beta_dist\n",
    "from scipy.stats import betabinom\n",
    "from scipy.stats import norm\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from typing import Dict, Any, Tuple\n",
    "from scipy import stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "@dataclass\n",
    "class SSBCResult:\n",
    "    alpha_target: float\n",
    "    alpha_corrected: float\n",
    "    u_star: int\n",
    "    n: int\n",
    "    satisfied_mass: float   # probability that coverage >= target\n",
    "    mode: Literal[\"beta\", \"beta-binomial\"]\n",
    "    details: Dict\n",
    "\n",
    "\n",
    "def ssbc_correct(\n",
    "    alpha_target: float,\n",
    "    n: int,\n",
    "    delta: float,\n",
    "    *,\n",
    "    mode: Literal[\"beta\", \"beta-binomial\"] = \"beta\",\n",
    "    m: Optional[int] = None,\n",
    "    bracket_width: Optional[int] = None,  # Δ in Algorithm 1\n",
    ") -> SSBCResult:\n",
    "    \"\"\"\n",
    "    Small-Sample Beta Correction (SSBC), corrected acceptance rule.\n",
    "    \n",
    "    Find the largest α' = u/(n+1) ≤ α_target such that:\n",
    "    P(Coverage(α') ≥ 1 - α_target) ≥ 1 - δ\n",
    "    \n",
    "    where Coverage(α') ~ Beta(n+1-u, u) for infinite test window.\n",
    "    \n",
    "    Args:\n",
    "        alpha_target: Target miscoverage rate\n",
    "        n: Calibration set size\n",
    "        delta: Risk tolerance (PAC parameter)\n",
    "        mode: \"beta\" for infinite test window, \"beta-binomial\" for finite\n",
    "        m: Test window size (for beta-binomial mode)\n",
    "        bracket_width: Search radius around initial guess (default: min(n, 10))\n",
    "    \"\"\"\n",
    "    if not (0.0 < alpha_target < 1.0):\n",
    "        raise ValueError(\"alpha_target must be in (0,1).\")\n",
    "    if n < 1:\n",
    "        raise ValueError(\"n must be >= 1.\")\n",
    "    if not (0.0 < delta < 1.0):\n",
    "        raise ValueError(\"delta must be in (0,1).\")\n",
    "    if mode not in (\"beta\", \"beta-binomial\"):\n",
    "        raise ValueError(\"mode must be 'beta' or 'beta-binomial'.\")\n",
    "    \n",
    "    # Maximum u to search (α' must be ≤ α_target)\n",
    "    u_max = min(n, math.floor(alpha_target * (n + 1)))\n",
    "    target_coverage = 1 - alpha_target\n",
    "    \n",
    "    # Initial guess for u using normal approximation to Beta distribution\n",
    "    # We want P(Beta(n+1-u, u) >= target_coverage) ≈ 1-δ\n",
    "    # Using normal approximation: u ≈ u_target - z_δ * sqrt(u_target)\n",
    "    # where u_target = (n+1)*α_target and z_δ = Φ^(-1)(1-δ)\n",
    "    u_target = (n + 1) * alpha_target\n",
    "    z_delta = norm.ppf(1 - delta)  # quantile function (inverse CDF)\n",
    "    u_star_guess = max(1, math.floor(u_target - z_delta * math.sqrt(u_target)))\n",
    "    \n",
    "    # Clamp to valid range\n",
    "    u_star_guess = min(u_max, u_star_guess)\n",
    "    \n",
    "    # Bracket width (Δ in Algorithm 1)\n",
    "    if bracket_width is None:\n",
    "        # Adaptive bracket: wider for small n, scales with √n for large n\n",
    "        # For large n, the uncertainty scales as √u_target ~ (n*α)^(1/2)\n",
    "        bracket_width = max(5, min(int(2 * z_delta * math.sqrt(u_target)), n // 10))\n",
    "        bracket_width = min(bracket_width, 100)  # cap at 100 for efficiency\n",
    "    \n",
    "    # Search bounds - ensure we don't go outside [1, u_max]\n",
    "    u_min = max(1, u_star_guess - bracket_width)\n",
    "    u_search_max = min(u_max, u_star_guess + bracket_width)\n",
    "    \n",
    "    # If the guess is way off (e.g., guess > u_max), fall back to full search\n",
    "    if u_min > u_search_max:\n",
    "        u_min = 1\n",
    "        u_search_max = u_max\n",
    "    \n",
    "    if mode == \"beta-binomial\":\n",
    "        m_eval = m if m is not None else n\n",
    "        if m_eval < 1:\n",
    "            raise ValueError(\"m must be >= 1 for beta-binomial mode.\")\n",
    "        k_thresh = math.ceil(target_coverage * m_eval)\n",
    "    \n",
    "    u_star = None\n",
    "    mass_star = None\n",
    "    \n",
    "    # Search from u_min up to u_search_max to find the largest u that satisfies the condition\n",
    "    # Keep updating u_star as we find larger values that work\n",
    "    search_log = []\n",
    "    for u in range(u_min, u_search_max + 1):\n",
    "        # When we calibrate at α' = u/(n+1), coverage follows:\n",
    "        a = n + 1 - u  # first parameter\n",
    "        b = u          # second parameter\n",
    "        alpha_prime = u / (n + 1)\n",
    "        \n",
    "        if mode == \"beta\":\n",
    "            # P(Coverage ≥ target_coverage) where Coverage ~ Beta(a, b)\n",
    "            # Using: P(X >= x) = 1 - CDF(x) for continuous distributions\n",
    "            ptail = 1 - beta_dist.cdf(target_coverage, a, b)\n",
    "        else:\n",
    "            # P(X ≥ k_thresh) where X ~ BetaBinomial(m, a, b)\n",
    "            ptail = betabinom.sf(k_thresh - 1, m_eval, a, b)\n",
    "        \n",
    "        passes = ptail >= 1 - delta\n",
    "        search_log.append({\n",
    "            'u': u,\n",
    "            'alpha_prime': alpha_prime,\n",
    "            'a': a,\n",
    "            'b': b,\n",
    "            'ptail': ptail,\n",
    "            'threshold': 1 - delta,\n",
    "            'passes': passes\n",
    "        })\n",
    "        \n",
    "        # Accept if probability is high enough - keep updating to find the largest\n",
    "        if passes:\n",
    "            u_star = u\n",
    "            mass_star = ptail\n",
    "    \n",
    "    # If nothing passes, fall back to u=1 (most conservative)\n",
    "    if u_star is None:\n",
    "        u_star = 1\n",
    "        a = n + 1 - u_star\n",
    "        b = u_star\n",
    "        mass_star = (1 - beta_dist.cdf(target_coverage, a, b)\n",
    "                     if mode == \"beta\" else\n",
    "                     betabinom.sf(k_thresh - 1, (m if m else n), a, b))\n",
    "    \n",
    "    alpha_corrected = u_star / (n + 1)\n",
    "    \n",
    "    return SSBCResult(\n",
    "        alpha_target=alpha_target,\n",
    "        alpha_corrected=alpha_corrected,\n",
    "        u_star=u_star,\n",
    "        n=n,\n",
    "        satisfied_mass=mass_star,\n",
    "        mode=mode,\n",
    "        details=dict(\n",
    "            u_max=u_max,\n",
    "            u_star_guess=u_star_guess,\n",
    "            search_range=(u_min, u_search_max),\n",
    "            bracket_width=bracket_width,\n",
    "            delta=delta,\n",
    "            m=(m if (mode == \"beta-binomial\") else None),\n",
    "            acceptance_rule=\"P(Coverage >= target) >= 1-delta\",\n",
    "            search_log=search_log,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "class BinaryClassifierSimulator:\n",
    "    def __init__(self, p_class1, beta_params_class0, beta_params_class1, seed=None):\n",
    "        \"\"\"\n",
    "        Simulate binary classification data with probabilities from Beta distributions.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        p_class1 : float\n",
    "            Probability of drawing class 1 (class imbalance parameter)\n",
    "        beta_params_class0 : tuple (a, b)\n",
    "            Beta distribution parameters for p(class=1) when true label is 0\n",
    "        beta_params_class1 : tuple (a, b)\n",
    "            Beta distribution parameters for p(class=1) when true label is 1\n",
    "        seed : int, optional\n",
    "            Random seed for reproducibility\n",
    "        \"\"\"\n",
    "        self.p_class1 = p_class1\n",
    "        self.p_class0 = 1.0 - p_class1\n",
    "        self.a0, self.b0 = beta_params_class0\n",
    "        self.a1, self.b1 = beta_params_class1\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "    \n",
    "    def generate(self, n_samples):\n",
    "        \"\"\"\n",
    "        Generate n_samples of (label, p(class=0), p(class=1))\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_samples : int\n",
    "            Number of samples to generate\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        labels : np.ndarray, shape (n_samples,)\n",
    "            True binary labels (0 or 1)\n",
    "        probs : np.ndarray, shape (n_samples, 2)\n",
    "            Classification probabilities [p(class=0), p(class=1)]\n",
    "        \"\"\"\n",
    "        # Draw true labels according to class distribution\n",
    "        labels = self.rng.choice([0, 1], size=n_samples, p=[self.p_class0, self.p_class1])\n",
    "        \n",
    "        # Initialize probability array\n",
    "        probs = np.zeros((n_samples, 2))\n",
    "        \n",
    "        # For each label, draw classification probability from appropriate Beta\n",
    "        for i, label in enumerate(labels):\n",
    "            if label == 0:\n",
    "                # True label is 0: sample p(class=1) from Beta(a0, b0)\n",
    "                p_class1 = self.rng.beta(self.a0, self.b0)\n",
    "            else:\n",
    "                # True label is 1: sample p(class=1) from Beta(a1, b1)\n",
    "                p_class1 = self.rng.beta(self.a1, self.b1)\n",
    "            \n",
    "            probs[i, 1] = p_class1  # p(class=1)\n",
    "            probs[i, 0] = 1.0 - p_class1  # p(class=0)\n",
    "        \n",
    "        return labels, probs\n",
    "\n",
    "def clopper_pearson_intervals(labels, confidence=0.95):\n",
    "    \"\"\"\n",
    "    Compute Clopper-Pearson (exact binomial) confidence intervals for class prevalences.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    labels : np.ndarray\n",
    "        Binary labels (0 or 1)\n",
    "    confidence : float, default=0.95\n",
    "        Confidence level (e.g., 0.95 for 95% CI)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    intervals : dict\n",
    "        Dictionary with keys 0 and 1, each containing:\n",
    "        - 'count': number of samples in this class\n",
    "        - 'proportion': observed proportion\n",
    "        - 'lower': lower bound of CI\n",
    "        - 'upper': upper bound of CI\n",
    "    \"\"\"\n",
    "    alpha = 1 - confidence\n",
    "    n_total = len(labels)\n",
    "    \n",
    "    intervals = {}\n",
    "    \n",
    "    for label in [0, 1]:\n",
    "        count = np.sum(labels == label)\n",
    "        proportion = count / n_total\n",
    "        \n",
    "        # Clopper-Pearson uses Beta distribution quantiles\n",
    "        # Lower bound: Beta(count, n-count+1) at alpha/2\n",
    "        # Upper bound: Beta(count+1, n-count) at 1-alpha/2\n",
    "        \n",
    "        if count == 0:\n",
    "            lower = 0.0\n",
    "            upper = stats.beta.ppf(1 - alpha/2, count + 1, n_total - count)\n",
    "        elif count == n_total:\n",
    "            lower = stats.beta.ppf(alpha/2, count, n_total - count + 1)\n",
    "            upper = 1.0\n",
    "        else:\n",
    "            lower = stats.beta.ppf(alpha/2, count, n_total - count + 1)\n",
    "            upper = stats.beta.ppf(1 - alpha/2, count + 1, n_total - count)\n",
    "        \n",
    "        intervals[label] = {\n",
    "            'count': count,\n",
    "            'proportion': proportion,\n",
    "            'lower': lower,\n",
    "            'upper': upper\n",
    "        }\n",
    "    \n",
    "    return intervals\n",
    "\n",
    "def split_by_class(labels, probs):\n",
    "    \"\"\"\n",
    "    Split calibration data by true class for Mondrian conformal prediction.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    labels : np.ndarray, shape (n,)\n",
    "        True binary labels (0 or 1)\n",
    "    probs : np.ndarray, shape (n, 2)\n",
    "        Classification probabilities [P(class=0), P(class=1)]\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    class_data : dict\n",
    "        Dictionary with keys 0 and 1, each containing:\n",
    "        - 'labels': labels for this class (all same value)\n",
    "        - 'probs': probabilities for samples in this class\n",
    "        - 'indices': original indices (for tracking)\n",
    "        - 'n': number of samples in this class\n",
    "    \"\"\"\n",
    "    class_data = {}\n",
    "    \n",
    "    for label in [0, 1]:\n",
    "        mask = labels == label\n",
    "        indices = np.where(mask)[0]\n",
    "        \n",
    "        class_data[label] = {\n",
    "            'labels': labels[mask],\n",
    "            'probs': probs[mask],\n",
    "            'indices': indices,\n",
    "            'n': np.sum(mask)\n",
    "        }\n",
    "    \n",
    "    return class_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac775fa8-1525-4614-9ee4-75476068dbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- quick sanity check ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Example: alpha=0.10, n=50, delta=0.10 (90% PAC)\n",
    "    print(\"=\"*60)\n",
    "    print(\"n=50 test:\")\n",
    "    print(\"=\"*60)\n",
    "    result_beta = ssbc_correct(0.10, n=50, delta=0.10, mode=\"beta\")\n",
    "    print(\"Beta mode:\")\n",
    "    print(f\"  alpha_target:    {result_beta.alpha_target}\")\n",
    "    print(f\"  alpha_corrected: {result_beta.alpha_corrected:.6f}\")\n",
    "    print(f\"  u_star:          {result_beta.u_star}\")\n",
    "    print(f\"  u_star_guess:    {result_beta.details['u_star_guess']}\")\n",
    "    print(f\"  search_range:    {result_beta.details['search_range']}\")\n",
    "    print(f\"  satisfied_mass:  {result_beta.satisfied_mass:.6f}\")\n",
    "    print(f\"\\n  Search log:\")\n",
    "    for entry in result_beta.details['search_log']:\n",
    "        status = \"✓ PASS\" if entry['passes'] else \"✗ FAIL\"\n",
    "        print(f\"    u={entry['u']}: α'={entry['alpha_prime']:.4f}, \"\n",
    "              f\"Beta({entry['a']},{entry['b']}), \"\n",
    "              f\"P(C≥0.9)={entry['ptail']:.4f} {status}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"n=5000 test:\")\n",
    "    print(\"=\"*60)\n",
    "    result_large = ssbc_correct(0.10, n=50000, delta=0.10, mode=\"beta\", bracket_width=15)\n",
    "    print(\"Beta mode:\")\n",
    "    print(f\"  alpha_target:    {result_large.alpha_target}\")\n",
    "    print(f\"  alpha_corrected: {result_large.alpha_corrected:.6f}\")\n",
    "    print(f\"  u_star:          {result_large.u_star}\")\n",
    "    print(f\"  u_star_guess:    {result_large.details['u_star_guess']}\")\n",
    "    print(f\"  search_range:    {result_large.details['search_range']}\")\n",
    "    print(f\"  satisfied_mass:  {result_large.satisfied_mass:.6f}\")\n",
    "    print(f\"  searched {len(result_large.details['search_log'])} values instead of 50\")\n",
    "    print(f\"\\n  Search log :\")\n",
    "    log = result_large.details['search_log']\n",
    "    for entry in log:\n",
    "        status = \"✓ PASS\" if entry['passes'] else \"✗ FAIL\"\n",
    "        print(f\"    u={entry['u']}: α'={entry['alpha_prime']:.4f}, \"\n",
    "              f\"Beta({entry['a']},{entry['b']}), \"\n",
    "              f\"P(C≥0.9)={entry['ptail']:.4f} {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc54fd4f-f8ad-483b-843d-1e1d2a24d213",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Test it\n",
    "if __name__ == \"__main__\":\n",
    "    # Simulate imbalanced data: 10% positive class\n",
    "    # Class 0: Beta(2, 8) → mean p(class=1) = 0.2 (low scores, correct)\n",
    "    # Class 1: Beta(8, 2) → mean p(class=1) = 0.8 (high scores, correct)\n",
    "    \n",
    "    sim = BinaryClassifierSimulator(\n",
    "        p_class1=0.10,\n",
    "        beta_params_class0=(2, 8),\n",
    "        beta_params_class1=(8, 2),\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    labels, probs = sim.generate(n_samples=50)\n",
    "    \n",
    "    print(f\"Shape of labels: {labels.shape}\")\n",
    "    print(f\"Shape of probs: {probs.shape}\")\n",
    "    print(f\"\\nFirst 5 samples:\")\n",
    "    for i in range(5):\n",
    "        print(f\"Label={labels[i]}, P(0)={probs[i,0]:.3f}, P(1)={probs[i,1]:.3f}\")\n",
    "    \n",
    "    print(f\"\\nClass balance: {np.bincount(labels)}\")\n",
    "    print(f\"Mean P(1) when true=0: {probs[labels==0, 1].mean():.3f}\")\n",
    "    print(f\"Mean P(1) when true=1: {probs[labels==1, 1].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c49b92e-a292-4073-9b7d-4552ad15a04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Test it with the simulator\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Generate data\n",
    "    sim = BinaryClassifierSimulator(\n",
    "        p_class1=0.10,\n",
    "        beta_params_class0=(2, 8),\n",
    "        beta_params_class1=(8, 2),\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    labels, probs = sim.generate(n_samples=100)\n",
    "    \n",
    "    # Compute Clopper-Pearson intervals\n",
    "    intervals = clopper_pearson_intervals(labels, confidence=0.95)\n",
    "    \n",
    "    print(\"Clopper-Pearson 95% Confidence Intervals:\")\n",
    "    print(\"=\" * 60)\n",
    "    for label in [0, 1]:\n",
    "        info = intervals[label]\n",
    "        print(f\"\\nClass {label}:\")\n",
    "        print(f\"  Count:      {info['count']}/{len(labels)}\")\n",
    "        print(f\"  Proportion: {info['proportion']:.4f}\")\n",
    "        print(f\"  95% CI:     [{info['lower']:.4f}, {info['upper']:.4f}]\")\n",
    "    \n",
    "    # Test edge cases\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Edge case tests:\")\n",
    "    \n",
    "    # All zeros\n",
    "    labels_zeros = np.zeros(50, dtype=int)\n",
    "    intervals_zeros = clopper_pearson_intervals(labels_zeros)\n",
    "    print(f\"\\nAll class 0: Class 1 interval = [{intervals_zeros[1]['lower']:.4f}, {intervals_zeros[1]['upper']:.4f}]\")\n",
    "    \n",
    "    # All ones\n",
    "    labels_ones = np.ones(50, dtype=int)\n",
    "    intervals_ones = clopper_pearson_intervals(labels_ones)\n",
    "    print(f\"All class 1: Class 0 interval = [{intervals_ones[0]['lower']:.4f}, {intervals_ones[0]['upper']:.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f284cf-5a4d-480a-8889-383e5ee1e7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mondrian_conformal_calibrate(class_data, alpha_target, delta, mode=\"beta\", m=None):\n",
    "    \"\"\"\n",
    "    Perform Mondrian (per-class) conformal calibration with SSBC correction.\n",
    "    \n",
    "    For each class, compute:\n",
    "    1. Nonconformity scores: s(x, y) = 1 - P(y|x)\n",
    "    2. SSBC-corrected alpha for PAC guarantee\n",
    "    3. Conformal quantile threshold\n",
    "    4. Singleton error rate bounds via PAC guarantee\n",
    "    \n",
    "    Then evaluate prediction set sizes on calibration data PER CLASS and MARGINALLY.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    class_data : dict\n",
    "        Output from split_by_class()\n",
    "    alpha_target : float or dict\n",
    "        Target miscoverage rate for each class\n",
    "        If float: same for both classes\n",
    "        If dict: {0: α0, 1: α1} for per-class control\n",
    "    delta : float or dict\n",
    "        PAC risk tolerance for each class\n",
    "        If float: same for both classes\n",
    "        If dict: {0: δ0, 1: δ1} for per-class control\n",
    "    mode : str\n",
    "        \"beta\" (infinite test) or \"beta-binomial\" (finite test)\n",
    "    m : int, optional\n",
    "        Test window size for beta-binomial mode\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    calibration_result : dict\n",
    "        Dictionary with keys 0 and 1, each containing calibration info\n",
    "        \n",
    "    prediction_stats : dict\n",
    "        Dictionary with keys:\n",
    "        - 0, 1: per-class statistics (conditioned on true label)\n",
    "        - 'marginal': overall statistics (ignoring true labels)\n",
    "    \"\"\"\n",
    "    # Handle scalar or dict inputs for alpha and delta\n",
    "    if isinstance(alpha_target, (int, float)):\n",
    "        alpha_dict = {0: alpha_target, 1: alpha_target}\n",
    "    else:\n",
    "        alpha_dict = alpha_target\n",
    "    \n",
    "    if isinstance(delta, (int, float)):\n",
    "        delta_dict = {0: delta, 1: delta}\n",
    "    else:\n",
    "        delta_dict = delta\n",
    "    \n",
    "    calibration_result = {}\n",
    "    \n",
    "    # Step 1: Calibrate per class\n",
    "    for label in [0, 1]:\n",
    "        data = class_data[label]\n",
    "        n = data['n']\n",
    "        alpha_class = alpha_dict[label]\n",
    "        delta_class = delta_dict[label]\n",
    "        \n",
    "        if n == 0:\n",
    "            calibration_result[label] = {\n",
    "                'n': 0,\n",
    "                'alpha_target': alpha_class,\n",
    "                'alpha_corrected': None,\n",
    "                'delta': delta_class,\n",
    "                'threshold': None,\n",
    "                'scores': np.array([]),\n",
    "                'ssbc_result': None,\n",
    "                'error': 'No calibration samples for this class'\n",
    "            }\n",
    "            continue\n",
    "        \n",
    "        # Compute nonconformity scores: s(x, y) = 1 - P(y|x)\n",
    "        true_class_probs = data['probs'][:, label]\n",
    "        scores = 1.0 - true_class_probs\n",
    "        \n",
    "        # Apply SSBC to get corrected alpha\n",
    "        ssbc_result = ssbc_correct(\n",
    "            alpha_target=alpha_class,\n",
    "            n=n,\n",
    "            delta=delta_class,\n",
    "            mode=mode,\n",
    "            m=m\n",
    "        )\n",
    "        \n",
    "        alpha_corrected = ssbc_result.alpha_corrected\n",
    "        \n",
    "        # Compute conformal quantile threshold\n",
    "        k = int(np.ceil((n + 1) * (1 - alpha_corrected)))\n",
    "        k = min(k, n)\n",
    "        \n",
    "        sorted_scores = np.sort(scores)\n",
    "        threshold = sorted_scores[k - 1] if k > 0 else sorted_scores[0]\n",
    "        \n",
    "        calibration_result[label] = {\n",
    "            'n': n,\n",
    "            'alpha_target': alpha_class,\n",
    "            'alpha_corrected': alpha_corrected,\n",
    "            'delta': delta_class,\n",
    "            'threshold': threshold,\n",
    "            'scores': sorted_scores,\n",
    "            'ssbc_result': ssbc_result,\n",
    "            'k': k\n",
    "        }\n",
    "    \n",
    "    # Step 2: Evaluate prediction sets\n",
    "    if calibration_result[0].get('threshold') is None or calibration_result[1].get('threshold') is None:\n",
    "        return calibration_result, {\n",
    "            'error': 'Cannot compute prediction sets - missing threshold for at least one class'\n",
    "        }\n",
    "    \n",
    "    threshold_0 = calibration_result[0]['threshold']\n",
    "    threshold_1 = calibration_result[1]['threshold']\n",
    "    \n",
    "    # Helper to compute Clopper-Pearson interval\n",
    "    def cp_interval(count, total, confidence=0.95):\n",
    "        alpha = 1 - confidence\n",
    "        proportion = count / total if total > 0 else 0.0\n",
    "        \n",
    "        if total == 0:\n",
    "            return {\n",
    "                'count': count,\n",
    "                'proportion': 0.0,\n",
    "                'lower': 0.0,\n",
    "                'upper': 0.0\n",
    "            }\n",
    "        \n",
    "        if count == 0:\n",
    "            lower = 0.0\n",
    "            upper = stats.beta.ppf(1 - alpha/2, count + 1, total - count)\n",
    "        elif count == total:\n",
    "            lower = stats.beta.ppf(alpha/2, count, total - count + 1)\n",
    "            upper = 1.0\n",
    "        else:\n",
    "            lower = stats.beta.ppf(alpha/2, count, total - count + 1)\n",
    "            upper = stats.beta.ppf(1 - alpha/2, count + 1, total - count)\n",
    "        \n",
    "        return {\n",
    "            'count': count,\n",
    "            'proportion': proportion,\n",
    "            'lower': lower,\n",
    "            'upper': upper\n",
    "        }\n",
    "    \n",
    "    prediction_stats = {}\n",
    "    \n",
    "    # Step 2a: Evaluate per true class\n",
    "    for true_label in [0, 1]:\n",
    "        data = class_data[true_label]\n",
    "        n_class = data['n']\n",
    "        \n",
    "        if n_class == 0:\n",
    "            prediction_stats[true_label] = {\n",
    "                'n_class': 0,\n",
    "                'error': 'No samples in this class'\n",
    "            }\n",
    "            continue\n",
    "        \n",
    "        probs = data['probs']\n",
    "        prediction_sets = []\n",
    "        \n",
    "        for i in range(n_class):\n",
    "            score_0 = 1.0 - probs[i, 0]\n",
    "            score_1 = 1.0 - probs[i, 1]\n",
    "            \n",
    "            pred_set = []\n",
    "            if score_0 <= threshold_0:\n",
    "                pred_set.append(0)\n",
    "            if score_1 <= threshold_1:\n",
    "                pred_set.append(1)\n",
    "            \n",
    "            prediction_sets.append(pred_set)\n",
    "        \n",
    "        # Count set sizes and correctness\n",
    "        n_abstentions = sum(len(ps) == 0 for ps in prediction_sets)\n",
    "        n_doublets = sum(len(ps) == 2 for ps in prediction_sets)\n",
    "        \n",
    "        n_singletons_correct = sum(ps == [true_label] for ps in prediction_sets)\n",
    "        n_singletons_incorrect = sum(len(ps) == 1 and true_label not in ps for ps in prediction_sets)\n",
    "        n_singletons_total = n_singletons_correct + n_singletons_incorrect\n",
    "        \n",
    "        # PAC bounds\n",
    "        n_escalations = n_doublets + n_abstentions\n",
    "        \n",
    "        if n_escalations > 0 and n_singletons_total > 0:\n",
    "            rho = n_singletons_total / n_escalations\n",
    "            kappa = n_abstentions / n_escalations\n",
    "            alpha_singlet_bound = alpha_dict[true_label] * (1 + 1/rho) - kappa/rho\n",
    "            alpha_singlet_observed = n_singletons_incorrect / n_singletons_total if n_singletons_total > 0 else 0.0\n",
    "        else:\n",
    "            rho = None\n",
    "            kappa = None\n",
    "            alpha_singlet_bound = None\n",
    "            alpha_singlet_observed = None\n",
    "        \n",
    "        prediction_stats[true_label] = {\n",
    "            'n_class': n_class,\n",
    "            'abstentions': cp_interval(n_abstentions, n_class),\n",
    "            'singletons': cp_interval(n_singletons_total, n_class),\n",
    "            'singletons_correct': cp_interval(n_singletons_correct, n_class),\n",
    "            'singletons_incorrect': cp_interval(n_singletons_incorrect, n_class),\n",
    "            'doublets': cp_interval(n_doublets, n_class),\n",
    "            'prediction_sets': prediction_sets,\n",
    "            'pac_bounds': {\n",
    "                'rho': rho,\n",
    "                'kappa': kappa,\n",
    "                'alpha_singlet_bound': alpha_singlet_bound,\n",
    "                'alpha_singlet_observed': alpha_singlet_observed,\n",
    "                'n_singletons': n_singletons_total,\n",
    "                'n_escalations': n_escalations\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    # Step 2b: MARGINAL ANALYSIS (ignoring true labels)\n",
    "    # Reconstruct full dataset\n",
    "    all_labels = np.concatenate([class_data[0]['labels'], class_data[1]['labels']])\n",
    "    all_probs = np.concatenate([class_data[0]['probs'], class_data[1]['probs']], axis=0)\n",
    "    all_indices = np.concatenate([class_data[0]['indices'], class_data[1]['indices']])\n",
    "    \n",
    "    # Sort back to original order\n",
    "    sort_idx = np.argsort(all_indices)\n",
    "    all_labels = all_labels[sort_idx]\n",
    "    all_probs = all_probs[sort_idx]\n",
    "    \n",
    "    n_total = len(all_labels)\n",
    "    \n",
    "    # Compute prediction sets for all samples\n",
    "    all_prediction_sets = []\n",
    "    for i in range(n_total):\n",
    "        score_0 = 1.0 - all_probs[i, 0]\n",
    "        score_1 = 1.0 - all_probs[i, 1]\n",
    "        \n",
    "        pred_set = []\n",
    "        if score_0 <= threshold_0:\n",
    "            pred_set.append(0)\n",
    "        if score_1 <= threshold_1:\n",
    "            pred_set.append(1)\n",
    "        \n",
    "        all_prediction_sets.append(pred_set)\n",
    "    \n",
    "    # Count overall set sizes\n",
    "    n_abstentions_total = sum(len(ps) == 0 for ps in all_prediction_sets)\n",
    "    n_singletons_total = sum(len(ps) == 1 for ps in all_prediction_sets)\n",
    "    n_doublets_total = sum(len(ps) == 2 for ps in all_prediction_sets)\n",
    "    \n",
    "    # Break down singletons by predicted class\n",
    "    n_singletons_pred_0 = sum(ps == [0] for ps in all_prediction_sets)\n",
    "    n_singletons_pred_1 = sum(ps == [1] for ps in all_prediction_sets)\n",
    "    \n",
    "    # Compute overall coverage\n",
    "    n_covered = sum(all_labels[i] in all_prediction_sets[i] for i in range(n_total))\n",
    "    coverage = n_covered / n_total\n",
    "    \n",
    "    # Compute errors on singletons\n",
    "    singleton_mask = [len(ps) == 1 for ps in all_prediction_sets]\n",
    "    n_singletons_covered = sum(\n",
    "        all_labels[i] in all_prediction_sets[i] \n",
    "        for i in range(n_total) \n",
    "        if singleton_mask[i]\n",
    "    )\n",
    "    n_singletons_errors = n_singletons_total - n_singletons_covered\n",
    "    \n",
    "    # Overall PAC bounds (using weighted average of alphas for interpretation)\n",
    "    # Note: This is approximate since we have different α per class\n",
    "    n_escalations_total = n_doublets_total + n_abstentions_total\n",
    "    \n",
    "    if n_escalations_total > 0 and n_singletons_total > 0:\n",
    "        rho_marginal = n_singletons_total / n_escalations_total\n",
    "        kappa_marginal = n_abstentions_total / n_escalations_total\n",
    "        \n",
    "        # Weighted average alpha (by class size)\n",
    "        n_0 = class_data[0]['n']\n",
    "        n_1 = class_data[1]['n']\n",
    "        alpha_weighted = (n_0 * alpha_dict[0] + n_1 * alpha_dict[1]) / (n_0 + n_1)\n",
    "        \n",
    "        alpha_singlet_bound_marginal = alpha_weighted * (1 + 1/rho_marginal) - kappa_marginal/rho_marginal\n",
    "        alpha_singlet_observed_marginal = n_singletons_errors / n_singletons_total\n",
    "    else:\n",
    "        rho_marginal = None\n",
    "        kappa_marginal = None\n",
    "        alpha_weighted = None\n",
    "        alpha_singlet_bound_marginal = None\n",
    "        alpha_singlet_observed_marginal = None\n",
    "    \n",
    "    prediction_stats['marginal'] = {\n",
    "        'n_total': n_total,\n",
    "        'coverage': {\n",
    "            'count': n_covered,\n",
    "            'rate': coverage,\n",
    "            'ci_95': cp_interval(n_covered, n_total)\n",
    "        },\n",
    "        'abstentions': cp_interval(n_abstentions_total, n_total),\n",
    "        'singletons': {\n",
    "            **cp_interval(n_singletons_total, n_total),\n",
    "            'pred_0': n_singletons_pred_0,\n",
    "            'pred_1': n_singletons_pred_1,\n",
    "            'errors': n_singletons_errors\n",
    "        },\n",
    "        'doublets': cp_interval(n_doublets_total, n_total),\n",
    "        'prediction_sets': all_prediction_sets,\n",
    "        'pac_bounds': {\n",
    "            'rho': rho_marginal,\n",
    "            'kappa': kappa_marginal,\n",
    "            'alpha_weighted': alpha_weighted,\n",
    "            'alpha_singlet_bound': alpha_singlet_bound_marginal,\n",
    "            'alpha_singlet_observed': alpha_singlet_observed_marginal,\n",
    "            'n_singletons': n_singletons_total,\n",
    "            'n_escalations': n_escalations_total\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return calibration_result, prediction_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53af542b-6f23-4d74-aa3e-06c93d826a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_prediction_stats(prediction_stats: Dict[Any, Any],\n",
    "                            calibration_result: Dict[Any, Any],\n",
    "                            verbose: bool = True) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Pretty/robust summary for Mondrian conformal prediction stats.\n",
    "\n",
    "    Tolerates multiple schema shapes:\n",
    "      - dicts with 'rate'/'ci_95' or 'proportion'/'lower'/'upper'\n",
    "      - raw ints for counts (e.g., marginal['singletons']['pred_0'] = 339)\n",
    "      - per-class singleton correct/incorrect either nested under 'singletons'\n",
    "        OR as top-level aliases 'singletons_correct' / 'singletons_incorrect'.\n",
    "\n",
    "    Also computes Clopper–Pearson CIs when missing, and splits marginal\n",
    "    singleton errors by predicted class.\n",
    "    \"\"\"\n",
    "\n",
    "    # ------------------------- helpers -------------------------\n",
    "    def cp_interval(count: int, total: int, confidence: float = 0.95) -> Dict[str, float]:\n",
    "        \"\"\"Clopper–Pearson exact CI using SciPy.\"\"\"\n",
    "        alpha = 1 - confidence\n",
    "        count = int(count)\n",
    "        total = int(total)\n",
    "        if total <= 0:\n",
    "            return {'count': count, 'proportion': 0.0, 'lower': 0.0, 'upper': 0.0}\n",
    "        p = count / total\n",
    "        if count == 0:\n",
    "            lower = 0.0\n",
    "            upper = stats.beta.ppf(1 - alpha/2, 1, total)\n",
    "        elif count == total:\n",
    "            lower = stats.beta.ppf(alpha/2, total, 1)\n",
    "            upper = 1.0\n",
    "        else:\n",
    "            lower = stats.beta.ppf(alpha/2, count, total - count + 1)\n",
    "            upper = stats.beta.ppf(1 - alpha/2, count + 1, total - count)\n",
    "        return {'count': count, 'proportion': float(p), 'lower': float(lower), 'upper': float(upper)}\n",
    "\n",
    "    def as_dict(x: Any) -> Dict[str, Any]:\n",
    "        return x if isinstance(x, dict) else {}\n",
    "\n",
    "    def get_count(x: Any, default: int = 0) -> int:\n",
    "        if isinstance(x, dict):\n",
    "            return int(x.get('count', default))\n",
    "        if isinstance(x, (int,)):\n",
    "            return int(x)\n",
    "        return default\n",
    "\n",
    "    def get_rate(x: Any, default: float = 0.0) -> float:\n",
    "        if isinstance(x, dict):\n",
    "            if 'rate' in x: return float(x['rate'])\n",
    "            if 'proportion' in x: return float(x['proportion'])\n",
    "            # Some producers put only count; caller must compute rate.\n",
    "            return default\n",
    "        if isinstance(x, (float,)):\n",
    "            return float(x)\n",
    "        return default\n",
    "\n",
    "    def get_ci_tuple(x: Any) -> Tuple[float, float]:\n",
    "        if not isinstance(x, dict):\n",
    "            return (0.0, 0.0)\n",
    "        if 'ci_95' in x and isinstance(x['ci_95'], (tuple, list)) and len(x['ci_95']) == 2:\n",
    "            return float(x['ci_95'][0]), float(x['ci_95'][1])\n",
    "        lo = x.get('lower', 0.0)\n",
    "        hi = x.get('upper', 0.0)\n",
    "        return float(lo), float(hi)\n",
    "\n",
    "    def ensure_ci(d: Dict[str, Any], count: int, total: int) -> Tuple[float, float, float]:\n",
    "        \"\"\"\n",
    "        Return (rate, lo, hi). If d already has rate/CI, use them; else compute CP from count/total.\n",
    "        \"\"\"\n",
    "        r = get_rate(d, default=None)\n",
    "        lo, hi = get_ci_tuple(d)\n",
    "        if r is None or (lo == 0.0 and hi == 0.0 and (count > 0 or total > 0)):\n",
    "            ci = cp_interval(count, total)\n",
    "            return ci['proportion'], ci['lower'], ci['upper']\n",
    "        return float(r), float(lo), float(hi)\n",
    "\n",
    "    def pct(x: float) -> str:\n",
    "        return f\"{x:6.2%}\"\n",
    "\n",
    "    summary: Dict[str, Any] = {}\n",
    "\n",
    "    if verbose:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"PREDICTION SET STATISTICS (All rates with 95% Clopper–Pearson CIs)\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "    # ----------------- per-class (conditioned on Y) -----------------\n",
    "    for class_label in [0, 1]:\n",
    "        if class_label not in prediction_stats:\n",
    "            continue\n",
    "        cls = prediction_stats[class_label]\n",
    "\n",
    "        if isinstance(cls, dict) and 'error' in cls:\n",
    "            if verbose:\n",
    "                print(f\"\\nClass {class_label}: {cls['error']}\")\n",
    "            summary[class_label] = {'error': cls['error']}\n",
    "            continue\n",
    "\n",
    "        n = int(cls.get('n', cls.get('n_class', 0)))\n",
    "        alpha_target = cls.get('alpha_target',\n",
    "                               calibration_result.get(class_label, {}).get('alpha_target', None))\n",
    "        delta = cls.get('delta',\n",
    "                        calibration_result.get(class_label, {}).get('delta', None))\n",
    "\n",
    "        abst = as_dict(cls.get('abstentions', {}))\n",
    "        sing = as_dict(cls.get('singletons', {}))\n",
    "        # Accept both nested and flat aliases\n",
    "        sing_corr = as_dict(sing.get('correct', cls.get('singletons_correct', {})))\n",
    "        sing_inc  = as_dict(sing.get('incorrect', cls.get('singletons_incorrect', {})))\n",
    "        doub = as_dict(cls.get('doublets', {}))\n",
    "        pac  = as_dict(cls.get('pac_bounds', {}))\n",
    "\n",
    "        # Counts\n",
    "        abst_count = get_count(abst)\n",
    "        sing_count = get_count(sing)\n",
    "        sing_corr_count = get_count(sing_corr)\n",
    "        sing_inc_count  = get_count(sing_inc)\n",
    "        doub_count = get_count(doub)\n",
    "\n",
    "        # Ensure rates/CIs (fallback to CP if missing)\n",
    "        abst_rate, abst_lo, abst_hi = ensure_ci(abst, abst_count, n)\n",
    "        sing_rate, sing_lo, sing_hi = ensure_ci(sing, sing_count, n)\n",
    "        sing_corr_rate, sing_corr_lo, sing_corr_hi = ensure_ci(sing_corr, sing_corr_count, n)\n",
    "        sing_inc_rate,  sing_inc_lo,  sing_inc_hi  = ensure_ci(sing_inc,  sing_inc_count,  n)\n",
    "        doub_rate, doub_lo, doub_hi = ensure_ci(doub, doub_count, n)\n",
    "\n",
    "        # P(error | singleton, Y=class)\n",
    "        err_given_single_ci = cp_interval(sing_inc_count, sing_count if sing_count > 0 else 1)\n",
    "\n",
    "        class_summary = {\n",
    "            'n': n,\n",
    "            'alpha_target': alpha_target,\n",
    "            'delta': delta,\n",
    "            'abstentions': {\n",
    "                'count': abst_count,\n",
    "                'rate': abst_rate,\n",
    "                'ci_95': (abst_lo, abst_hi)\n",
    "            },\n",
    "            'singletons': {\n",
    "                'count': sing_count,\n",
    "                'rate': sing_rate,\n",
    "                'ci_95': (sing_lo, sing_hi),\n",
    "                'correct': {\n",
    "                    'count': sing_corr_count,\n",
    "                    'rate': sing_corr_rate,\n",
    "                    'ci_95': (sing_corr_lo, sing_corr_hi)\n",
    "                },\n",
    "                'incorrect': {\n",
    "                    'count': sing_inc_count,\n",
    "                    'rate': sing_inc_rate,\n",
    "                    'ci_95': (sing_inc_lo, sing_inc_hi)\n",
    "                },\n",
    "                'error_given_singleton': {\n",
    "                    'count': sing_inc_count,\n",
    "                    'denom': sing_count,\n",
    "                    'rate': err_given_single_ci['proportion'],\n",
    "                    'ci_95': (err_given_single_ci['lower'], err_given_single_ci['upper'])\n",
    "                }\n",
    "            },\n",
    "            'doublets': {\n",
    "                'count': doub_count,\n",
    "                'rate': doub_rate,\n",
    "                'ci_95': (doub_lo, doub_hi)\n",
    "            },\n",
    "            'pac_bounds': pac\n",
    "        }\n",
    "        summary[class_label] = class_summary\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"CLASS {class_label} (Conditioned on True Label = {class_label})\")\n",
    "            print(f\"{'='*80}\")\n",
    "            alpha_str = f\"{alpha_target:.3f}\" if alpha_target is not None else \"n/a\"\n",
    "            delta_str = f\"{delta:.3f}\" if delta is not None else \"n/a\"\n",
    "            print(f\"  n={n}, α_target={alpha_str}, δ={delta_str}\")\n",
    "\n",
    "            print(\"\\nPrediction Set Breakdown:\")\n",
    "            print(f\"  Abstentions:  {abst_count:4d} / {n:4d} = {pct(abst_rate)}  \"\n",
    "                  f\"95% CI: [{abst_lo:.4f}, {abst_hi:.4f}]\")\n",
    "            print(f\"  Singletons:   {sing_count:4d} / {n:4d} = {pct(sing_rate)}  \"\n",
    "                  f\"95% CI: [{sing_lo:.4f}, {sing_hi:.4f}]\")\n",
    "            print(f\"    ├─ Correct:   {sing_corr_count:4d} / {n:4d} = {pct(sing_corr_rate)}  \"\n",
    "                  f\"95% CI: [{sing_corr_lo:.4f}, {sing_corr_hi:.4f}]\")\n",
    "            print(f\"    └─ Incorrect: {sing_inc_count:4d} / {n:4d} = {pct(sing_inc_rate)}  \"\n",
    "                  f\"95% CI: [{sing_inc_lo:.4f}, {sing_inc_hi:.4f}]\")\n",
    "\n",
    "            print(f\"  Singleton error | Y={class_label}: \"\n",
    "                  f\"{sing_inc_count:4d} / {sing_count:4d} = {pct(err_given_single_ci['proportion'])}  \"\n",
    "                  f\"95% CI: [{err_given_single_ci['lower']:.4f}, {err_given_single_ci['upper']:.4f}]\")\n",
    "\n",
    "            print(f\"\\n  Doublets:     {doub_count:4d} / {n:4d} = {pct(doub_rate)}  \"\n",
    "                  f\"95% CI: [{doub_lo:.4f}, {doub_hi:.4f}]\")\n",
    "\n",
    "            if pac and pac.get('rho', None) is not None:\n",
    "                print(f\"\\n  PAC Singleton Error Rate (δ={delta_str}):\")\n",
    "                print(f\"    ρ = {pac.get('rho', 0):.3f}, κ = {pac.get('kappa', 0):.3f}\")\n",
    "                if 'alpha_singlet_bound' in pac and 'alpha_singlet_observed' in pac:\n",
    "                    bound = float(pac['alpha_singlet_bound'])\n",
    "                    observed = float(pac['alpha_singlet_observed'])\n",
    "                    ok = '✓' if observed <= bound else '✗'\n",
    "                    print(f\"    α'_bound:    {bound:.4f}\")\n",
    "                    print(f\"    α'_observed: {observed:.4f} {ok}\")\n",
    "\n",
    "    # ----------------- marginal / deployment view -----------------\n",
    "    if 'marginal' in prediction_stats:\n",
    "        marg = prediction_stats['marginal']\n",
    "        n_total = int(marg['n_total'])\n",
    "\n",
    "        cov    = as_dict(marg.get('coverage', {}))\n",
    "        abst_m = as_dict(marg.get('abstentions', {}))\n",
    "        sing_m = as_dict(marg.get('singletons', {}))\n",
    "        doub_m = as_dict(marg.get('doublets', {}))\n",
    "        pac_m  = as_dict(marg.get('pac_bounds', {}))\n",
    "\n",
    "        cov_count = get_count(cov)\n",
    "        abst_m_count = get_count(abst_m)\n",
    "        sing_total = get_count(sing_m)\n",
    "        doub_m_count = get_count(doub_m)\n",
    "\n",
    "        cov_rate, cov_lo, cov_hi = ensure_ci(cov, cov_count, n_total)\n",
    "        abst_m_rate, abst_m_lo, abst_m_hi = ensure_ci(abst_m, abst_m_count, n_total)\n",
    "        sing_m_rate, sing_m_lo, sing_m_hi = ensure_ci(sing_m, sing_total, n_total)\n",
    "        doub_m_rate, doub_m_lo, doub_m_hi = ensure_ci(doub_m, doub_m_count, n_total)\n",
    "\n",
    "        # pred_0 / pred_1 may be dicts or ints (counts)\n",
    "        raw_s0 = sing_m.get('pred_0', 0)\n",
    "        raw_s1 = sing_m.get('pred_1', 0)\n",
    "        s0_count = get_count(raw_s0)\n",
    "        s1_count = get_count(raw_s1)\n",
    "        # prefer provided rate/CI, else compute off n_total\n",
    "        if isinstance(raw_s0, dict):\n",
    "            s0_rate, s0_lo, s0_hi = ensure_ci(raw_s0, s0_count, n_total)\n",
    "        else:\n",
    "            s0_ci = cp_interval(s0_count, n_total)\n",
    "            s0_rate, s0_lo, s0_hi = s0_ci['proportion'], s0_ci['lower'], s0_ci['upper']\n",
    "        if isinstance(raw_s1, dict):\n",
    "            s1_rate, s1_lo, s1_hi = ensure_ci(raw_s1, s1_count, n_total)\n",
    "        else:\n",
    "            s1_ci = cp_interval(s1_count, n_total)\n",
    "            s1_rate, s1_lo, s1_hi = s1_ci['proportion'], s1_ci['lower'], s1_ci['upper']\n",
    "\n",
    "        # overall singleton errors (dict or int). Denominator should be sing_total.\n",
    "        raw_s_err = sing_m.get('errors', 0)\n",
    "        s_err_count = get_count(raw_s_err)\n",
    "        if isinstance(raw_s_err, dict):\n",
    "            s_err_rate, s_err_lo, s_err_hi = ensure_ci(raw_s_err, s_err_count, sing_total if sing_total > 0 else 1)\n",
    "        else:\n",
    "            se_ci = cp_interval(s_err_count, sing_total if sing_total > 0 else 1)\n",
    "            s_err_rate, s_err_lo, s_err_hi = se_ci['proportion'], se_ci['lower'], se_ci['upper']\n",
    "\n",
    "        # errors by predicted class via per-class incorrect singletons\n",
    "        # (pred 0 errors happen when Y=1 singleton is wrong; pred 1 errors when Y=0 singleton is wrong)\n",
    "        err_pred0_count = int(prediction_stats.get(1, {}).get('singletons', {})\n",
    "                              .get('incorrect', prediction_stats.get(1, {}).get('singletons_incorrect', {}))\n",
    "                              .get('count', 0))\n",
    "        err_pred1_count = int(prediction_stats.get(0, {}).get('singletons', {})\n",
    "                              .get('incorrect', prediction_stats.get(0, {}).get('singletons_incorrect', {}))\n",
    "                              .get('count', 0))\n",
    "        pred0_err_ci = cp_interval(err_pred0_count, s0_count if s0_count > 0 else 1)\n",
    "        pred1_err_ci = cp_interval(err_pred1_count, s1_count if s1_count > 0 else 1)\n",
    "\n",
    "        marginal_summary = {\n",
    "            'n_total': n_total,\n",
    "            'coverage': {\n",
    "                'count': cov_count,\n",
    "                'rate': cov_rate,\n",
    "                'ci_95': (cov_lo, cov_hi)\n",
    "            },\n",
    "            'abstentions': {\n",
    "                'count': abst_m_count,\n",
    "                'rate': abst_m_rate,\n",
    "                'ci_95': (abst_m_lo, abst_m_hi)\n",
    "            },\n",
    "            'singletons': {\n",
    "                'count': sing_total,\n",
    "                'rate': sing_m_rate,\n",
    "                'ci_95': (sing_m_lo, sing_m_hi),\n",
    "                'pred_0': {'count': s0_count, 'rate': s0_rate, 'ci_95': (s0_lo, s0_hi)},\n",
    "                'pred_1': {'count': s1_count, 'rate': s1_rate, 'ci_95': (s1_lo, s1_hi)},\n",
    "                'errors':  {'count': s_err_count, 'rate': s_err_rate, 'ci_95': (s_err_lo, s_err_hi)},\n",
    "                'errors_by_pred': {\n",
    "                    'pred_0': {\n",
    "                        'count': err_pred0_count,\n",
    "                        'denom': s0_count,\n",
    "                        'rate': pred0_err_ci['proportion'],\n",
    "                        'ci_95': (pred0_err_ci['lower'], pred0_err_ci['upper'])\n",
    "                    },\n",
    "                    'pred_1': {\n",
    "                        'count': err_pred1_count,\n",
    "                        'denom': s1_count,\n",
    "                        'rate': pred1_err_ci['proportion'],\n",
    "                        'ci_95': (pred1_err_ci['lower'], pred1_err_ci['upper'])\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            'doublets': {\n",
    "                'count': doub_m_count,\n",
    "                'rate': doub_m_rate,\n",
    "                'ci_95': (doub_m_lo, doub_m_hi)\n",
    "            },\n",
    "            'pac_bounds': pac_m\n",
    "        }\n",
    "        summary['marginal'] = marginal_summary\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(\"MARGINAL ANALYSIS (Deployment View - Ignores True Labels)\")\n",
    "            print(f\"{'='*80}\")\n",
    "            print(f\"  Total samples: {n_total}\")\n",
    "\n",
    "            print(\"\\nOverall Coverage:\")\n",
    "            print(f\"  Covered: {cov_count:4d} / {n_total:4d} = {pct(cov_rate)}  \"\n",
    "                  f\"95% CI: [{cov_lo:.4f}, {cov_hi:.4f}]\")\n",
    "\n",
    "            print(\"\\nPrediction Set Distribution:\")\n",
    "            print(f\"  Abstentions: {abst_m_count:4d} / {n_total:4d} = {pct(abst_m_rate)}  \"\n",
    "                  f\"95% CI: [{abst_m_lo:.4f}, {abst_m_hi:.4f}]\")\n",
    "            print(f\"  Singletons:  {sing_total:4d} / {n_total:4d} = {pct(sing_m_rate)}  \"\n",
    "                  f\"95% CI: [{sing_m_lo:.4f}, {sing_m_hi:.4f}]\")\n",
    "            print(f\"    ├─ Pred 0: {s0_count:4d} / {n_total:4d} = {pct(s0_rate)}  \"\n",
    "                  f\"95% CI: [{s0_lo:.4f}, {s0_hi:.4f}]\")\n",
    "            print(f\"    ├─ Pred 1: {s1_count:4d} / {n_total:4d} = {pct(s1_rate)}  \"\n",
    "                  f\"95% CI: [{s1_lo:.4f}, {s1_hi:.4f}]\")\n",
    "            print(f\"    ├─ Errors (overall): {s_err_count:4d} / {sing_total:4d} = {pct(s_err_rate)}  \"\n",
    "                  f\"95% CI: [{s_err_lo:.4f}, {s_err_hi:.4f}]\")\n",
    "            print(f\"    ├─ Pred 0 errors:    {err_pred0_count:4d} / {s0_count:4d} = {pct(pred0_err_ci['proportion'])}  \"\n",
    "                  f\"95% CI: [{pred0_err_ci['lower']:.4f}, {pred0_err_ci['upper']:.4f}]\")\n",
    "            print(f\"    └─ Pred 1 errors:    {err_pred1_count:4d} / {s1_count:4d} = {pct(pred1_err_ci['proportion'])}  \"\n",
    "                  f\"95% CI: [{pred1_err_ci['lower']:.4f}, {pred1_err_ci['upper']:.4f}]\")\n",
    "\n",
    "            print(f\"  Doublets:    {doub_m_count:4d} / {n_total:4d} = {pct(doub_m_rate)}  \"\n",
    "                  f\"95% CI: [{doub_m_lo:.4f}, {doub_m_hi:.4f}]\")\n",
    "\n",
    "            if pac_m and pac_m.get('rho', None) is not None:\n",
    "                aw = pac_m.get('alpha_weighted', None)\n",
    "                aw_str = f\"{float(aw):.3f}\" if aw is not None else \"n/a\"\n",
    "                print(f\"\\n  Overall PAC Bounds (weighted α={aw_str}):\")\n",
    "                print(f\"    ρ = {pac_m.get('rho', 0):.3f}, κ = {pac_m.get('kappa', 0):.3f}\")\n",
    "                if 'alpha_singlet_bound' in pac_m and 'alpha_singlet_observed' in pac_m:\n",
    "                    bound = float(pac_m['alpha_singlet_bound'])\n",
    "                    observed = float(pac_m['alpha_singlet_observed'])\n",
    "                    ok = '✓' if observed <= bound else '✗'\n",
    "                    print(f\"    α'_bound:    {bound:.4f}\")\n",
    "                    print(f\"    α'_observed: {observed:.4f} {ok}\")\n",
    "\n",
    "                n_escalations = int(pac_m.get('n_escalations',\n",
    "                                              doub_m_count + abst_m_count))\n",
    "                print(f\"\\n  Deployment Decision Mix:\")\n",
    "                print(f\"    Automate: {sing_total} singletons ({sing_m_rate:.1%})\")\n",
    "                print(f\"    Escalate: {n_escalations} doublets+abstentions \"\n",
    "                      f\"({n_escalations / n_total:.1%})\")\n",
    "\n",
    "    return summary\n",
    "\n",
    "\n",
    "# Test it\n",
    "if __name__ == \"__main__\":\n",
    "    sim = BinaryClassifierSimulator(\n",
    "        p_class1=0.50,\n",
    "        beta_params_class0=(2, 7),  # Class 0: low P(1) scores\n",
    "        beta_params_class1=(7, 2),  # Class 1: high P(1) scores\n",
    "    )\n",
    "    \n",
    "    labels, probs = sim.generate(n_samples=1000)\n",
    "    \n",
    "    # Split by class\n",
    "    class_data = split_by_class(labels, probs)\n",
    "\n",
    "    plt.hist( class_data[0]['probs'][:,1], bins=np.linspace(0,1,50))\n",
    "    plt.hist( class_data[1]['probs'][:,1], bins=np.linspace(0,1,50), alpha=0.5)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Mondrian Split Summary:\")\n",
    "    print(\"=\" * 60)\n",
    "    for label in [0, 1]:\n",
    "        data = class_data[label]\n",
    "        print(f\"\\nClass {label}:\")\n",
    "        print(f\"  n = {data['n']}\")\n",
    "        print(f\"  Indices (first 5): {data['indices'][:5]}\")\n",
    "        print(f\"  Labels unique: {np.unique(data['labels'])}\")\n",
    "        print(f\"  Mean P({label}): {data['probs'][:, label].mean():.3f}\")\n",
    "        print(f\"  Mean P({1-label}): {data['probs'][:, 1-label].mean():.3f}\")\n",
    "    \n",
    "    # Verify split is complete and non-overlapping\n",
    "    total_samples = sum(class_data[i]['n'] for i in [0, 1])\n",
    "    print(f\"\\nTotal samples after split: {total_samples} (should be {len(labels)})\")\n",
    "    \n",
    "    # Show example samples from each class\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Example samples from each class:\")\n",
    "    for label in [0, 1]:\n",
    "        data = class_data[label]\n",
    "        print(f\"\\nClass {label} (first 3 samples):\")\n",
    "        for i in range(min(3, data['n'])):\n",
    "            print(f\"  Sample {data['indices'][i]}: \"\n",
    "                  f\"Label={data['labels'][i]}, \"\n",
    "                  f\"P(0)={data['probs'][i, 0]:.3f}, \"\n",
    "                  f\"P(1)={data['probs'][i, 1]:.3f}\")\n",
    "\n",
    "    print()\n",
    "    print()\n",
    "    print()\n",
    "    print()\n",
    "        \n",
    "    cal_result, pred_stats = mondrian_conformal_calibrate(\n",
    "        class_data=class_data,\n",
    "        alpha_target={0: 0.10, 1: 0.10},\n",
    "        delta={0: 0.10, 1: 0.10},\n",
    "        mode=\"beta\"\n",
    "    )\n",
    "    \n",
    "    # Generate report with ALL Clopper-Pearson CIs\n",
    "    summary = report_prediction_stats(pred_stats, cal_result, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547b1779-f6d7-4dbb-aa0a-892e1874d100",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_0 = np.arange(0.05, 0.20, 0.05)\n",
    "delta_1 = np.arange(0.05, 0.20, 0.05)\n",
    "alpha_0 = np.arange(0.05, 0.20, 0.05) \n",
    "alpha_1 = np.arange(0.05, 0.20, 0.05) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632ad2d2-cde7-4f79-8b37-9e139d5acde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- your libs must already expose these:\n",
    "# from your_module import mondrian_conformal_calibrate, report_prediction_stats\n",
    "\n",
    "def sweep_hyperparams_and_collect(\n",
    "    class_data,\n",
    "    alpha_0, delta_0,\n",
    "    alpha_1, delta_1,\n",
    "    mode=\"beta\",\n",
    "    extra_metrics=None,\n",
    "    quiet=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Sweep (a0,d0,a1,d1), run mondrian_conformal_calibrate + report_prediction_stats,\n",
    "    and return a tidy DataFrame with hyperparams + selected metrics.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    combos = list(itertools.product(alpha_0, delta_0, alpha_1, delta_1))\n",
    "    for (a0, d0, a1, d1) in combos:\n",
    "        if not quiet:\n",
    "            print(f\"a0={a0:.3f}, d0={d0:.3f}, a1={a1:.3f}, d1={d1:.3f}\")\n",
    "        cal_result, pred_stats = mondrian_conformal_calibrate(\n",
    "            class_data=class_data,\n",
    "            alpha_target={0: float(a0), 1: float(a1)},\n",
    "            delta={0: float(d0), 1: float(d1)},\n",
    "            mode=mode\n",
    "        )\n",
    "        summary = report_prediction_stats(pred_stats, cal_result, verbose=False)\n",
    "\n",
    "        # robust getter\n",
    "        def g(d, *keys, default=None):\n",
    "            cur = d\n",
    "            for k in keys:\n",
    "                if not isinstance(cur, dict) or k not in cur:\n",
    "                    return default\n",
    "                cur = cur[k]\n",
    "            return cur\n",
    "\n",
    "        n_total   = int(g(summary, 'marginal', 'n_total', default=0) or 0)\n",
    "        cov       = float(g(summary, 'marginal', 'coverage', 'rate', default=0.0) or 0.0)\n",
    "        sing_rate = float(g(summary, 'marginal', 'singletons', 'rate', default=0.0) or 0.0)\n",
    "        sing_cnt  = int(g(summary, 'marginal', 'singletons', 'count', default=0) or 0)\n",
    "        abst_cnt  = int(g(summary, 'marginal', 'abstentions', 'count', default=0) or 0)\n",
    "        doub_cnt  = int(g(summary, 'marginal', 'doublets', 'count', default=0) or 0)\n",
    "        esc_rate  = (abst_cnt + doub_cnt) / float(n_total if n_total else 1)\n",
    "\n",
    "        err_all   = float(g(summary, 'marginal', 'singletons', 'errors', 'rate', default=0.0) or 0.0)\n",
    "        err_p0    = float(g(summary, 'marginal', 'singletons', 'errors_by_pred', 'pred_0', 'rate', default=0.0) or 0.0)\n",
    "        err_p1    = float(g(summary, 'marginal', 'singletons', 'errors_by_pred', 'pred_1', 'rate', default=0.0) or 0.0)\n",
    "\n",
    "        err_y0    = float(g(summary, 0, 'singletons', 'error_given_singleton', 'rate', default=0.0) or 0.0)\n",
    "        err_y1    = float(g(summary, 1, 'singletons', 'error_given_singleton', 'rate', default=0.0) or 0.0)\n",
    "\n",
    "        row = {\n",
    "            'a0': float(a0), 'd0': float(d0),\n",
    "            'a1': float(a1), 'd1': float(d1),\n",
    "            'cov': cov,\n",
    "            'sing_rate': sing_rate,\n",
    "            'err_all': err_all,\n",
    "            'err_pred0': err_p0,\n",
    "            'err_pred1': err_p1,\n",
    "            'err_y0': err_y0,\n",
    "            'err_y1': err_y1,\n",
    "            'esc_rate': esc_rate,\n",
    "            'n_total': int(n_total),\n",
    "            'sing_count': int(sing_cnt),\n",
    "            # handy extras for hover:\n",
    "            'm_abst': abst_cnt, 'm_doublets': doub_cnt,\n",
    "        }\n",
    "\n",
    "        if extra_metrics:\n",
    "            for name, fn in extra_metrics.items():\n",
    "                try:\n",
    "                    row[name] = fn(summary)\n",
    "                except Exception:\n",
    "                    row[name] = np.nan\n",
    "\n",
    "        rows.append(row)\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    return df.sort_values(['a0','d0','a1','d1'], kind='mergesort').reset_index(drop=True)\n",
    "\n",
    "\n",
    "# ------------------------ Plotly interactive chart ------------------------\n",
    "\n",
    "def plot_parallel_coordinates_plotly(\n",
    "    df,\n",
    "    columns=None,\n",
    "    color='err_all',\n",
    "    color_continuous_scale=None,\n",
    "    title=\"Mondrian sweep – interactive parallel coordinates\",\n",
    "    height=600,\n",
    "    base_opacity=0.9,        # opacity of the active / selected lines\n",
    "    unselected_opacity=0.06  # fade level for unselected lines\n",
    "):\n",
    "    import plotly.express as px\n",
    "\n",
    "    if columns is None:\n",
    "        default_cols = ['a0','d0','a1','d1',\n",
    "                        'cov','sing_rate',\n",
    "                        'err_all','err_pred0','err_pred1',\n",
    "                        'err_y0','err_y1',\n",
    "                        'esc_rate']\n",
    "        columns = [c for c in default_cols if c in df.columns]\n",
    "\n",
    "    fig = px.parallel_coordinates(\n",
    "        df,\n",
    "        dimensions=columns,\n",
    "        color=color if color in df.columns else None,\n",
    "        color_continuous_scale=color_continuous_scale or px.colors.sequential.Blugrn,\n",
    "        labels={c: c for c in columns},\n",
    "    )\n",
    "\n",
    "    # Maximize contrast between selected and unselected lines\n",
    "    if fig.data:\n",
    "        # Fade unselected lines\n",
    "        fig.data[0].unselected.update(line=dict(color=f\"rgba(1,1,1,{float(unselected_opacity)})\"))\n",
    "        # Make selected lines as opaque as possible (can’t change width)\n",
    "        # We can only set the *trace* line color/scale; bump overall alpha by\n",
    "        # moving the colorbar to fully opaque range.\n",
    "        # No direct line.opacity for parcoords; use colorscale without alpha.\n",
    "        pass\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        height=height,\n",
    "        margin=dict(l=40, r=40, t=60, b=40),\n",
    "        plot_bgcolor=\"white\",\n",
    "        paper_bgcolor=\"white\",\n",
    "        font=dict(size=14),\n",
    "        uirevision=True  # keep user brushing across updates\n",
    "    )\n",
    "\n",
    "    # Make axis labels and ranges more readable\n",
    "    fig.update_traces(\n",
    "        labelfont=dict(size=14),\n",
    "        rangefont=dict(size=12),\n",
    "        tickfont=dict(size=12)\n",
    "    )\n",
    "\n",
    "    # Optional: title for colorbar if we’re coloring by a column\n",
    "    if color in df.columns and fig.data and getattr(fig.data[0], \"line\", None):\n",
    "        if getattr(fig.data[0].line, \"colorbar\", None) is not None:\n",
    "            fig.data[0].line.colorbar.title = color\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "\n",
    "# -------- convenience wrapper: run sweep + show plotly figure --------\n",
    "\n",
    "def sweep_and_plot_parallel_plotly(\n",
    "    class_data,\n",
    "    delta_0, delta_1,\n",
    "    alpha_0, alpha_1,\n",
    "    mode=\"beta\",\n",
    "    extra_metrics=None,\n",
    "    color='err_all',\n",
    "    color_continuous_scale=None,\n",
    "    title=None,\n",
    "    height=600,\n",
    "):\n",
    "    df = sweep_hyperparams_and_collect(\n",
    "        class_data=class_data,\n",
    "        alpha_0=alpha_0, delta_0=delta_0,\n",
    "        alpha_1=alpha_1, delta_1=delta_1,\n",
    "        mode=mode,\n",
    "        extra_metrics=extra_metrics,\n",
    "        quiet=True\n",
    "    )\n",
    "    \n",
    "    fig = plot_parallel_coordinates_plotly(\n",
    "        df,\n",
    "        color=color,\n",
    "        color_continuous_scale=color_continuous_scale,\n",
    "        title=title,\n",
    "        height=height\n",
    "    )\n",
    "    return df, fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c752b67-1ef9-45b0-aee9-5ac53374bf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_0 = np.arange(0.05, 0.20, 0.05)\n",
    "delta_1 = np.arange(0.05, 0.20, 0.05)\n",
    "alpha_0 = np.arange(0.05, 0.20, 0.05)\n",
    "alpha_1 = np.arange(0.05, 0.20, 0.05)\n",
    "\n",
    "df, fig = sweep_and_plot_parallel_plotly(\n",
    "    class_data=class_data,\n",
    "    delta_0=delta_0, delta_1=delta_1,\n",
    "    alpha_0=alpha_0, alpha_1=alpha_1,\n",
    "    mode=\"beta\",\n",
    "    color='a1',  # try 'esc_rate' or 'cov' too\n",
    ")\n",
    "\n",
    "# in a notebook:\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b217e1-17cf-4da1-bb75-1acf9805fae3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
